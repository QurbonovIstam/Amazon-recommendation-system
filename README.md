# üõí –ì–∏–±—Ä–∏–¥–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ç–æ–≤–∞—Ä—ã Amazon  

## üîó Google Colab Notebook
[Open in Colab](https://colab.research.google.com/drive/1uny-9RoI_jlYNkgP2TemxGQ0rcAtEdSQ?usp=drive_link)

**Hybrid Recommendation System for Amazon Product Reviews**

---

## üá∑üá∫ –û –ø—Ä–æ–µ–∫—Ç–µ

### üéØ –¶–µ–ª—å
–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–≤–∞—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –∏—Å—Ç–æ—Ä–∏–∏ –æ—Ü–µ–Ω–æ–∫ –∏ –æ—Ç–∑—ã–≤–æ–≤.

### üìÇ –î–∞—Ç–∞—Å–µ—Ç
- –ò—Å—Ç–æ—á–Ω–∏–∫: [Kaggle ‚Äî Amazon Product Reviews](https://www.kaggle.com/datasets)  
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: **568,454**  
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: **10**  
- –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è: `Id`, `ProductId`, `UserId`, `ProfileName`, `HelpfulnessNumerator`, `HelpfulnessDenominator`, `Score`, `Time`, `Summary`, `Text`  

### üîç –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- –£–¥–∞–ª–µ–Ω—ã –∑–∞–ø–∏—Å–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.  
- –û—Ç–æ–±—Ä–∞–Ω—ã —Ç–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å ‚â•5 –æ—Ç–∑—ã–≤–∞–º–∏.  
- –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ `train` –∏ `test` –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É **Leave-One-Out** ‚Äî –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–∫—É–ø–∫–∞ –≤ —Ç–µ—Å—Ç, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤ –æ–±—É—á–µ–Ω–∏–µ.

---

## üß† –ü–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

### 1Ô∏è‚É£ **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å** ‚Äî –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (ALS)
- –ú–µ—Ç–æ–¥: Alternating Least Squares (–∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `implicit`)  
- –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: `factors=100`, `regularization=0.01`, `iterations=25`  
- –†–µ–∑—É–ª—å—Ç–∞—Ç: **Hit@10 = 0.4542**  

### 2Ô∏è‚É£ **–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å (Summary)** ‚Äî ALS + TF-IDF –Ω–∞ –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö
- –õ–æ–≥–∏–∫–∞: –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ø-20 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É —Å —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º TF-IDF –Ω–∞ `Summary`.  
- –†–µ–∑—É–ª—å—Ç–∞—Ç: **Hit@10 = 0.2408** (—É—Ö—É–¥—à–µ–Ω–∏–µ, –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º —à—É–º–Ω—ã–µ).  

### 3Ô∏è‚É£ **–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å (Full Text)** ‚Äî ALS + TF-IDF –Ω–∞ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö –æ—Ç–∑—ã–≤–æ–≤
- –õ–æ–≥–∏–∫–∞: –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–π, –Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `Text` (–ø–æ–ª–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤).  
- –†–µ–∑—É–ª—å—Ç–∞—Ç: **Hit@10 = 0.4667** (—É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —á–∏—Å—Ç—ã–º ALS).  

---

## üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–æ–¥–µ–ª—å                | –û–ø–∏—Å–∞–Ω–∏–µ                                     | Hit@10  |
|-----------------------|----------------------------------------------|---------|
| **Original ALS**      | –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è                   | 0.4542  |
| **Hybrid (Summary)**  | ALS + TF-IDF –Ω–∞ –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö             | 0.2408  |
| **Hybrid (Full Text)**| ALS + TF-IDF –Ω–∞ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö                | 0.4667  |

---

## üìà –í—ã–≤–æ–¥—ã
- –ß–∏—Å—Ç–∞—è –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ‚Äî —Å–∏–ª—å–Ω–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å.
- –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤ –Ω–µ—Å—É—Ç –ø–æ–ª–µ–∑–Ω—ã–π —Å–∏–≥–Ω–∞–ª –∏ —Å–ø–æ—Å–æ–±–Ω—ã —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
- –ö—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è (`Summary`) —Å–ª–∏—à–∫–æ–º –∑–∞—à—É–º–ª–µ–Ω—ã.

---

## üöÄ –í–æ–∑–º–æ–∂–Ω—ã–µ –¥–∞–ª—å–Ω–µ–π—à–∏–µ —à–∞–≥–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **BERT / RoBERTa** –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–º–µ—Ä–æ–º —Ç–æ–ø–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—Ç–æ–ø-50, —Ç–æ–ø-100).
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Å–∞ –ø—Ä–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–æ–∫ ALS –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.

---

## üîó Google Colab Notebook
[–û—Ç–∫—Ä—ã—Ç—å –ø—Ä–æ–µ–∫—Ç –≤ Colab](https://colab.research.google.com/drive/1uny-9RoI_jlYNkgP2TemxGQ0rcAtEdSQ?usp=drive_link)

---

## üá¨üáß English Version

### üéØ Goal
To build a hybrid recommendation system for predicting the next product a user might purchase based on their review history and product ratings.

### üìÇ Dataset
- Source: [Kaggle ‚Äî Amazon Product Reviews](https://www.kaggle.com/datasets)  
- Records: **568,454**  
- Features: **10**  
- Key columns: `Id`, `ProductId`, `UserId`, `ProfileName`, `HelpfulnessNumerator`, `HelpfulnessDenominator`, `Score`, `Time`, `Summary`, `Text`

### üîç Preprocessing
- Removed missing values.  
- Filtered users with ‚â•5 reviews.  
- Train/test split using **Leave-One-Out** ‚Äî last purchase in test set.

---

## üß† Models

### 1Ô∏è‚É£ **Baseline** ‚Äî Collaborative Filtering (ALS)
- Method: Alternating Least Squares (`implicit` library)  
- Hyperparameters: `factors=100`, `regularization=0.01`, `iterations=25`  
- Result: **Hit@10 = 0.4542**

### 2Ô∏è‚É£ **Hybrid (Summary)** ‚Äî ALS + TF-IDF on product summaries
- Reranked top-20 ALS candidates using cosine similarity with average TF-IDF vector from summaries.  
- Result: **Hit@10 = 0.2408** (worse due to noisy summaries).

### 3Ô∏è‚É£ **Hybrid (Full Text)** ‚Äî ALS + TF-IDF on full review texts
- Same logic, but using TF-IDF vectors from `Text`.  
- Result: **Hit@10 = 0.4667** (improved over ALS baseline).

---

## üìä Final Results

| Model                 | Description                                  | Hit@10  |
|-----------------------|----------------------------------------------|---------|
| **Original ALS**      | Collaborative Filtering                      | 0.4542  |
| **Hybrid (Summary)**  | ALS + TF-IDF on summaries                     | 0.2408  |
| **Hybrid (Full Text)**| ALS + TF-IDF on full review texts             | 0.4667  |

---

## üìà Conclusions
- Baseline ALS is strong, but hybrid models can outperform it.
- Full review texts provide valuable signal for reranking.
- Summaries are too noisy to be useful.

---

## üöÄ Future Work
- Use **BERT / RoBERTa** for semantic embeddings.
- Experiment with candidate list size (top-50, top-100).
- Adjust blending weights between ALS and content-based scores.

---


